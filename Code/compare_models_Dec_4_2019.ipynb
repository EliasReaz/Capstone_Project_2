{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <p style=\"font-family: Arial; font-size:2.0em; color:#3323C0; font-style:bold\">\n",
    "# Summary: Compare model results and final model selection</p>\n",
    "# <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model results and final model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Evaluate saved models both \"logisitic regression\" and \"random fores\" on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that model on the holdout test set\n",
    "\n",
    "** **last update: December 18, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "# from sklearn.metrics import plot_precision_recall_curve\n",
    "from time import time\n",
    "\n",
    "\n",
    "test_features = pd.read_csv('X_test_df_2.csv')\n",
    "test_labels = pd.read_csv('y_test_df_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.drop(columns=[\"views\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1213, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1213, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Models: linear regression and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for mdl in ['LR_2', 'RF_3']:\n",
    "    models[mdl] = joblib.load('{}.pkl'.format(mdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR_2': LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "           n_jobs=None, penalty='l2', random_state=42, solver='newton-cg',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " 'RF_3': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=4, min_samples_split=5,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=None,\n",
       "             oob_score=True, random_state=42, verbose=0, warm_start=False)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2.0em; color:#3323C0; font-style:bold\">\n",
    "Evaluate models on the validation set </p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred, average='micro'), 3)\n",
    "    recall = round(recall_score(labels, pred, average='micro'), 3)\n",
    "    \n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} /'.format(name, accuracy,precision, recall))\n",
    "\n",
    "    \n",
    "    print()\n",
    "    print(\"Classification report for \", name)\n",
    "    print()\n",
    "    print(classification_report(labels, pred))\n",
    "\n",
    "    \n",
    "#     print()\n",
    "#     print(\"Confusion matrix for \", name)\n",
    "#     print()\n",
    "#     print(confusion_matrix(labels, pred)) \n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_2 -- Accuracy: 0.594 / Precision: 0.594 / Recall: 0.594 /\n",
      "\n",
      "Classification report for  LR_2\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.66      0.70       302\n",
      "           1       0.48      0.43      0.46       295\n",
      "           2       0.64      0.79      0.71       305\n",
      "           3       0.50      0.49      0.49       311\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1213\n",
      "   macro avg       0.59      0.59      0.59      1213\n",
      "weighted avg       0.59      0.59      0.59      1213\n",
      "\n",
      "RF_3 -- Accuracy: 0.742 / Precision: 0.742 / Recall: 0.742 /\n",
      "\n",
      "Classification report for  RF_3\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84       302\n",
      "           1       0.74      0.61      0.67       295\n",
      "           2       0.76      0.82      0.79       305\n",
      "           3       0.65      0.67      0.66       311\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1213\n",
      "   macro avg       0.74      0.74      0.74      1213\n",
      "weighted avg       0.74      0.74      0.74      1213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name, mdl, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <p style=\"font-family: Arial; font-size:2.0em; color:#3323C0; font-style:bold\">\n",
    "# Class encoding -- 0:extereme, 1:high, 2:low, 3:medium </p>\n",
    "# <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* YouTube videos in the US are classified based on their number of views. \n",
    "* Videos are labeled as \"Extreme\", of which the number of views is more than 75 percentile. Videos with number of views between the 50th and 75th percentile is labeled as \"High\". Similarly, \"Medium\" is in between the 25th and 50th percentile of views and \"Low\" is below the 25th percentile.\n",
    "* Label endocing refers to these classes as Extreme:0, High:1, Low:2, Medium:3.\n",
    "* Two machine learning models, Logistic Regression and Random Forest are utilized.\n",
    "* Dataset is randomly split into two segments: training dataset (80% of the dataset) and test dataset (20% of the dataset).\n",
    "* Model parameters are chosen by using training dataset with five-fold cross-validation.  \n",
    "* In the Logistic Regression model, the regularization parameter C is chosen by using grid search technique.\n",
    "* In the Random Forest model, hyperparameters,  such as number of trees, maximum number of features, max. number of levels in each tree, are chosen by using random search technique. Random search instead of grid search is used to reduce the running time to find these hyperparameters. \n",
    "* Resuls demomstrate that random forest, with an accuracy of 74%, outperforms logistic regression, with an accuracy of 59%.  \n",
    "* In terms of precision, random forest has a precision of 83% for class 0 (extermely views video), while logistic regression has a precision of 75%. Precision is the ability to correctly diagonize positive   \n",
    "* Reasons that random forest performs better are:\n",
    "    * Random forest is one of the most well-known ensemble models that combines a large number of independent decision trees and is trained over random and equally distributed subsets of a dataset.\n",
    "    * Random forest offers less overfitting.\n",
    "    * Logistic regression is a linear model and works better where there exists a linear relationship. However, in the YouTube dataset the relationship between input features and output labels is non-linear, and hence the random forest performs better than the logistic regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
